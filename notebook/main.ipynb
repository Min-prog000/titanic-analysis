{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(minplu): GitHubフォルダ全体を移動する\n",
    "# 1\n",
    "\n",
    "# TODO(minplu): 提出ファイルのフォルダのパスを変更する\n",
    "# 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from copy import deepcopy\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from enum import Enum, auto\n",
    "from logging import (\n",
    "    DEBUG,\n",
    "    FileHandler,\n",
    "    Formatter,\n",
    "    Logger,\n",
    "    StreamHandler,\n",
    "    getLogger,\n",
    ")\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# 定数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データパス\n",
    "PATH_TRAIN = \"data\\\\titanic\\\\train.csv\"\n",
    "PATH_TEST = \"data\\\\titanic\\\\test.csv\"\n",
    "\n",
    "# ログファイルが保存されるフォルダのパス\n",
    "PATH_LOG = \"log\"\n",
    "\n",
    "# csvファイル保存用手法名\n",
    "LOGISTIC_REGRESSION = \"logreg\"\n",
    "RANDOM_FOREST = \"rf\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# クラス"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanFillMethod(Enum):\n",
    "    \"\"\"欠損値の置換方法の識別用Enumクラス\n",
    "\n",
    "    Args:\n",
    "        Enum: 継承するクラス\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    MEAN = auto()\n",
    "    MEDIAN = auto()\n",
    "    MODE = auto()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## ユーティリティー"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### 表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplayUtility:\n",
    "    \"\"\"データ表示用のユーティリティクラス\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def output_divider(title: str) -> None:\n",
    "        \"\"\"区切り線を出力する\n",
    "\n",
    "        Args:\n",
    "            title (str): 区切り線の中央に表示する文字列\n",
    "\n",
    "        \"\"\"\n",
    "        print(f\"-------------------- {title} --------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### ファイル操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FolderUtility:\n",
    "    \"\"\"フォルダ・ファイル用ユーティリティークラス\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def make_folder_if_not_exist(save_folder_name: Path) -> None:\n",
    "        \"\"\"同名のフォルダがなければ作成する\n",
    "\n",
    "        Args:\n",
    "            save_folder_name (str): フォルダパス\n",
    "\n",
    "        \"\"\"\n",
    "        if not save_folder_name.exists():\n",
    "            save_folder_name.mkdir()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### ロガー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(minplu): ロガーの出力メッセージをカスタマイズする\n",
    "# 4\n",
    "\n",
    "\n",
    "class LogSetting:\n",
    "    \"\"\"ログ設定用のクラス\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def set_logger(cls, log_folder_path: Path) -> Logger:\n",
    "        \"\"\"ロガーの出力設定を行う\n",
    "\n",
    "        Args:\n",
    "            log_folder_path (Path): ログファイルの保存先フォルダパス\n",
    "\n",
    "        Returns:\n",
    "            Logger: ログを出力するためのロガーオブジェクト\n",
    "\n",
    "        \"\"\"\n",
    "        logger = getLogger(__name__)\n",
    "\n",
    "        stream_handler = cls._generate_stream_handler()\n",
    "\n",
    "        FolderUtility.make_folder_if_not_exist(log_folder_path)\n",
    "        file_handler = cls._generate_file_handler()\n",
    "\n",
    "        logger.addHandler(file_handler)\n",
    "        logger.addHandler(stream_handler)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    @classmethod\n",
    "    def _generate_stream_handler(cls) -> StreamHandler:\n",
    "        \"\"\"ロガーのストリームハンドラーを生成する\n",
    "\n",
    "        Returns:\n",
    "            StreamHandler: ロガーのストリームハンドラー\n",
    "\n",
    "        \"\"\"\n",
    "        stream_handler_log_format = (\n",
    "            \"%(asctime)s %(name)s [%(levelname)s] %(funcName)s: %(message)s\"\n",
    "        )\n",
    "\n",
    "        stream_handler = StreamHandler()\n",
    "        stream_handler.setLevel(DEBUG)\n",
    "        stream_formatter = Formatter(stream_handler_log_format)\n",
    "\n",
    "        stream_handler.setFormatter(stream_formatter)\n",
    "\n",
    "        return stream_handler\n",
    "\n",
    "    @classmethod\n",
    "    def _generate_file_handler(cls) -> FileHandler:\n",
    "        \"\"\"ロガーのファイルハンドラーを生成する\n",
    "\n",
    "        Returns:\n",
    "            FileHandler: ロガーのファイルハンドラー\n",
    "\n",
    "        \"\"\"\n",
    "        jst = timezone(timedelta(hours=+9), \"JST\")\n",
    "        log_datetime = f\"{datetime.now(jst):%Y%m%d%H%M%S}\"\n",
    "\n",
    "        file_handler = FileHandler(rf\"{PATH_LOG}\\log_{log_datetime}.log\")\n",
    "        file_handler.setLevel(DEBUG)\n",
    "        file_handler_log_format = (\n",
    "            \"%(asctime)s %(name)s [%(levelname)s] %(funcName)s: %(message)s\"\n",
    "        )\n",
    "        file_formatter = Formatter(file_handler_log_format)\n",
    "\n",
    "        file_handler.setFormatter(file_formatter)\n",
    "\n",
    "        return file_handler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalysisUtility:\n",
    "    \"\"\"データセット確認用ユーティリティークラス\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def display_summary(cls, dataset: pd.DataFrame) -> None:\n",
    "        \"\"\"データフレームの概要を出力する\n",
    "\n",
    "        出力内容\n",
    "        - データセット本体\n",
    "        - 統計量（describe()）\n",
    "        - 欠損値かどうか（isnull()）\n",
    "        - 各列の欠損値の合計（isnull().sum()）\n",
    "\n",
    "        それぞれディバイダ―付きで表示する\n",
    "\n",
    "        Args:\n",
    "            dataset (pd.DataFrame): 出力対象のデータフレーム\n",
    "\n",
    "        \"\"\"\n",
    "        cls._display_data(dataset)\n",
    "        cls._display_statistics(dataset)\n",
    "        cls._display_is_nan(dataset)\n",
    "        cls._display_nan_sum(dataset)\n",
    "\n",
    "    @classmethod\n",
    "    def _display_data(cls, dataset: pd.DataFrame) -> None:\n",
    "        \"\"\"データフレームのデータを表示する\n",
    "\n",
    "        Args:\n",
    "            dataset (pd.DataFrame): 表示対象のデータフレーム\n",
    "\n",
    "        \"\"\"\n",
    "        DisplayUtility.output_divider(\"Data\")\n",
    "        display(dataset)\n",
    "\n",
    "    @classmethod\n",
    "    def _display_statistics(cls, dataset: pd.DataFrame) -> None:\n",
    "        \"\"\"データフレームの各列の統計量を表示する\n",
    "\n",
    "        Args:\n",
    "            dataset (pd.DataFrame): 表示対象のデータフレーム\n",
    "\n",
    "        \"\"\"\n",
    "        DisplayUtility.output_divider(\"Statistics\")\n",
    "        display(dataset.describe())\n",
    "\n",
    "    @classmethod\n",
    "    def _display_is_nan(cls, dataset: pd.DataFrame) -> None:\n",
    "        \"\"\"データフレームのデータのうち欠損値のみTrue、それ以外をFalseで表示する\n",
    "\n",
    "        Args:\n",
    "            dataset (pd.DataFrame): 表示対象のデータフレーム\n",
    "\n",
    "        \"\"\"\n",
    "        DisplayUtility.output_divider(\"Is NaN\")\n",
    "        display(dataset.isna())\n",
    "\n",
    "    @classmethod\n",
    "    def _display_nan_sum(cls, dataset: pd.DataFrame) -> None:\n",
    "        \"\"\"データフレームの各列の欠損値の合計を表示する\n",
    "\n",
    "        Args:\n",
    "            dataset (pd.DataFrame): 表示対象のデータフレーム\n",
    "\n",
    "        \"\"\"\n",
    "        DisplayUtility.output_divider(\"Sum of NaN\")\n",
    "        display(cls._calculate_nan_sum(dataset))\n",
    "\n",
    "    @classmethod\n",
    "    def _calculate_nan_sum(cls, dataset: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"データフレームの各列の欠損値の合計を計算する\n",
    "\n",
    "        Args:\n",
    "            dataset (pd.DataFrame): 計算対象のデータフレーム\n",
    "\n",
    "        Returns:\n",
    "            pd.Series[int]: 各列の欠損値の合計\n",
    "\n",
    "        \"\"\"\n",
    "        return dataset.isna().sum()\n",
    "\n",
    "    @classmethod\n",
    "    def display_categorized_columns(cls, dataset: pd.DataFrame) -> None:\n",
    "        \"\"\"データフレームの各列のユニークなデータとその数をdisplayメソッドで表示する\n",
    "\n",
    "        Args:\n",
    "            dataset (pd.DataFrame): 表示対象のデータフレーム\n",
    "\n",
    "        \"\"\"\n",
    "        columns = dataset.columns\n",
    "        for column in columns:\n",
    "            df_categorized = cls._categorize(dataset=dataset, column=column)\n",
    "            display(df_categorized)\n",
    "\n",
    "    @classmethod\n",
    "    def _categorize(cls, dataset: pd.DataFrame, column: str) -> pd.Series:\n",
    "        \"\"\"データフレームの列をユニークなデータごとにグルーピングする\n",
    "\n",
    "        Args:\n",
    "            dataset (pd.DataFrame): グルーピング対象のデータフレーム\n",
    "            column (str): グルーピング対象の列\n",
    "\n",
    "        Returns:\n",
    "            pd.Series: グルーピング後の一次元データ\n",
    "\n",
    "        \"\"\"\n",
    "        df_groupby = dataset.groupby(column)\n",
    "        df_categorized = df_groupby.size()\n",
    "\n",
    "        return df_categorized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### データセットの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessUtility:\n",
    "    \"\"\"前処理用ユーティリティークラス\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess_dataset(\n",
    "        cls,\n",
    "        dataset: pd.DataFrame,\n",
    "        selected_columns: list[str],\n",
    "        encode_columns: list[str],\n",
    "        logger: Logger,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"データセットの前処理を実行するクラス\n",
    "\n",
    "        1. 特徴量の抽出\n",
    "        2. カテゴリ変数のエンコード\n",
    "        3. 欠損値の置換\n",
    "\n",
    "        Args:\n",
    "            dataset (pd.DataFrame): 処理対象のデータフレーム\n",
    "            selected_columns (List[str]): 抽出対象の列\n",
    "            encode_columns (List[str]): エンコード対象の列\n",
    "            logger (Logger): ロガー\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: 処理後のデータフレーム\n",
    "\n",
    "        \"\"\"\n",
    "        # 1. 特徴量の抽出\n",
    "        dataset_selected = cls.select_features(dataset, selected_columns)\n",
    "\n",
    "        # 2. カテゴリ変数のエンコード\n",
    "        dataset_encoded = cls.encode_by_one_hot(dataset_selected, encode_columns)\n",
    "\n",
    "        # 3. 欠損値の置換（平均値）\n",
    "        dataset_filled = cls._fill_nan(\n",
    "            df=dataset_encoded,\n",
    "            nan_fill_method=NanFillMethod.MEAN,\n",
    "            round_figure=1,\n",
    "            logger=logger,\n",
    "        )\n",
    "\n",
    "        return dataset_filled\n",
    "\n",
    "    @classmethod\n",
    "    def select_features(\n",
    "        cls,\n",
    "        dataset: pd.DataFrame,\n",
    "        selected_columns: list[str],\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"データセットから必要な列を抽出する\n",
    "\n",
    "        Args:\n",
    "            dataset (pd.DataFrame): 抽出対象のデータフレーム\n",
    "            selected_columns (List[str]): 抽出する列\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: 抽出後のデータフレーム\n",
    "\n",
    "        \"\"\"\n",
    "        dataset_selected = dataset.loc[:, selected_columns]\n",
    "        return dataset_selected\n",
    "\n",
    "    # ワンホットエンコーディング\n",
    "    @classmethod\n",
    "    def encode_by_one_hot(\n",
    "        cls,\n",
    "        df: pd.DataFrame,\n",
    "        encoding_column_list: list[str],\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"データフレームにワンホットエンコーディングを実行する\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): エンコード対象のデータフレーム\n",
    "            encoding_column_list (List[str]): エンコード対象の列\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: エンコード後のデータフレーム\n",
    "\n",
    "        \"\"\"\n",
    "        df_encoded = deepcopy(df)\n",
    "        for column in encoding_column_list:\n",
    "            # エンコードしたデータフレームの取得\n",
    "            df_dummy = pd.get_dummies(df[column], dtype=int, prefix=column)\n",
    "\n",
    "            # 挿入位置の取得（エンコードする列の番号）\n",
    "            insert_location = df_encoded.columns.get_loc(column)\n",
    "\n",
    "            # 列の削除\n",
    "            df_encoded = df_encoded.drop(column, axis=1)\n",
    "\n",
    "            # データフレームの挿入\n",
    "            df_encoded = cls.insert_dataframe(df_encoded, df_dummy, insert_location)\n",
    "\n",
    "        return df_encoded\n",
    "\n",
    "    @classmethod\n",
    "    def insert_dataframe(\n",
    "        cls,\n",
    "        df_base: pd.DataFrame,\n",
    "        df_insert: pd.DataFrame,\n",
    "        insert_location: int | slice | np.ndarray,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"特定のデータフレームを別のデータフレームに挿入する\n",
    "\n",
    "        Args:\n",
    "            df_base (pd.DataFrame): 挿入先のデータフレーム\n",
    "            df_insert (pd.DataFrame): 挿入対象のデータフレーム\n",
    "            insert_location (Union[int, slice, np.ndarray]): 挿入位置\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: 挿入後のデータフレーム\n",
    "\n",
    "        \"\"\"\n",
    "        # 同名の列があった場合はdf_baseを返す\n",
    "        if not set(df_base.columns).isdisjoint(df_insert.columns):\n",
    "            print(\"There is a same column between df_base and df_insert.\")\n",
    "            return df_base\n",
    "\n",
    "        # 挿入位置から左のデータフレーム\n",
    "        df_divided_left = df_base.iloc[:, :insert_location]\n",
    "\n",
    "        # 挿入位置から右のデータフレーム\n",
    "        df_divided_right = df_base.iloc[:, insert_location:]\n",
    "\n",
    "        # 結合したデータフレーム\n",
    "        df_merged = pd.concat([df_divided_left, df_insert, df_divided_right], axis=1)\n",
    "\n",
    "        return df_merged\n",
    "\n",
    "    @classmethod\n",
    "    def _fill_nan(\n",
    "        cls,\n",
    "        df: pd.DataFrame,\n",
    "        nan_fill_method: NanFillMethod,\n",
    "        round_figure: int,\n",
    "        logger: Logger,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"欠損値（NaN）を置換する\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): 欠損値のあるデータフレーム\n",
    "            nan_fill_method (NanFillMethod): 欠損値の置換方法（平均値のみ有効）\n",
    "            round_figure (int): 置換に使用する値の有効桁数（小数点以下）\n",
    "            logger (Logger): エラー出力用のロガー\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame:\n",
    "            置換後のデータフレーム\n",
    "            nan_fill_methodが予期しない値の場合はdfをそのまま返す\n",
    "\n",
    "        \"\"\"\n",
    "        if nan_fill_method == NanFillMethod.MEAN:\n",
    "            fill_values = df.mean(numeric_only=True)\n",
    "            fill_values_round = round(fill_values, round_figure)\n",
    "            df_nan_filled = df.fillna(fill_values_round)\n",
    "            return df_nan_filled\n",
    "\n",
    "        logger.warning(\n",
    "            msg=\"Incorrect method inputted: please choose from class NanFillMethod.\",\n",
    "        )\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### CSV出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CsvUtility:\n",
    "    \"\"\"提出用CSVファイルのユーティリティークラス\"\"\"\n",
    "\n",
    "    # csv出力\n",
    "    @classmethod\n",
    "    def output_csv(cls, df: pd.DataFrame, postfix_method_name: str) -> None:\n",
    "        \"\"\"データフレームをcsvファイルに出力する\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): csvファイルにするデータフレーム\n",
    "            postfix_method_name (str): csvファイルの接尾辞に使用する手法名\n",
    "\n",
    "        \"\"\"\n",
    "        jst = timezone(timedelta(hours=+9), \"JST\")\n",
    "        train_datetime = datetime.now(jst)\n",
    "\n",
    "        # 保存先フォルダ名の接尾辞（日付）\n",
    "        save_folder_name = cls._generate_save_folder_name(train_datetime)\n",
    "        save_folder_path = Path(save_folder_name)\n",
    "\n",
    "        # 保存先フォルダの作成\n",
    "        FolderUtility.make_folder_if_not_exist(save_folder_path)\n",
    "\n",
    "        # 保存ファイル名の接尾辞（日付と日時）\n",
    "        save_file_name = cls._generate_save_file_name(\n",
    "            postfix_method_name,\n",
    "            train_datetime,\n",
    "        )\n",
    "\n",
    "        # 保存ファイルのパス（カレントディレクトリの直下に作成する）\n",
    "        save_path = cls._generate_save_path(save_folder_name, save_file_name)\n",
    "\n",
    "        df.to_csv(save_path, index=False)\n",
    "\n",
    "    @classmethod\n",
    "    def _generate_save_folder_name(cls, train_datetime: datetime) -> str:\n",
    "        \"\"\"ファイル保存先フォルダパスを生成する\n",
    "\n",
    "        Args:\n",
    "            train_datetime (datetime): フォルダの作成日時\n",
    "\n",
    "        Returns:\n",
    "            str: ファイル保存先フォルダパス\n",
    "\n",
    "        \"\"\"\n",
    "        postfix_save_folder_name = train_datetime.strftime(\"%Y%m%d\")\n",
    "        # 保存先フォルダ名\n",
    "        save_folder_name = f\"submission_{postfix_save_folder_name}\"\n",
    "        return save_folder_name\n",
    "\n",
    "    @classmethod\n",
    "    def _generate_save_file_name(\n",
    "        cls,\n",
    "        postfix_method_name: str,\n",
    "        train_datetime: datetime,\n",
    "    ) -> str:\n",
    "        \"\"\"保存ファイル名を生成する\n",
    "\n",
    "        Args:\n",
    "            postfix_method_name (str): ファイル名の末尾につける学習手法の名前\n",
    "            train_datetime (datetime): ファイルの作成日時\n",
    "\n",
    "        Returns:\n",
    "            str: 保存ファイル名\n",
    "\n",
    "        \"\"\"\n",
    "        postfix_datetime = train_datetime.strftime(\"%Y%m%d%H%M%S\")\n",
    "        # 保存ファイル名\n",
    "        save_file_name = f\"submission_{postfix_method_name}_{postfix_datetime}.csv\"\n",
    "\n",
    "        return save_file_name\n",
    "\n",
    "    @classmethod\n",
    "    def _generate_save_path(cls, save_folder_name: str, save_file_name: str) -> str:\n",
    "        \"\"\"ファイル保存先パスを生成する\n",
    "\n",
    "        Args:\n",
    "            save_folder_name (str): ファイル保存先フォルダ名\n",
    "            save_file_name (str): 保存ファイル名\n",
    "\n",
    "        Returns:\n",
    "            str: ファイル保存先パス\n",
    "\n",
    "        \"\"\"\n",
    "        save_path = f\"{save_folder_name}/{save_file_name}\"\n",
    "        return save_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 例外"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FalseComponentError(Exception):\n",
    "    \"\"\"二つの配列の行列数が異なる場合に呼び出す例外クラス\n",
    "\n",
    "    Args:\n",
    "        Exception: 継承する例外クラス\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, msg: str) -> None:\n",
    "        \"\"\"コンストラクタ\n",
    "\n",
    "        Args:\n",
    "            msg (str): 例外発生時に出力する文字列\n",
    "\n",
    "        \"\"\"\n",
    "        self.msg = msg\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"文字列を出力する場合に呼び出される関数\n",
    "\n",
    "        Args:\n",
    "            msg (str): 出力する例外の内容\n",
    "\n",
    "        Returns:\n",
    "            str: 出力する例外の内容\n",
    "\n",
    "        \"\"\"\n",
    "        return self.msg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# Logger生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_log = Path(PATH_LOG)\n",
    "logger = LogSetting.set_logger(path_log)\n",
    "\n",
    "logger.debug(\"Logger has been set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# データセット分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## 事前準備\n",
    "---\n",
    "- pd.set_option\n",
    "\n",
    "  pd.DataFrame型の表示行列数の変更\n",
    "\n",
    "  - 第一引数\n",
    "\n",
    "    変更する表示方向\n",
    "\n",
    "    - display.max_rows\n",
    "\n",
    "      表示行数\n",
    "\n",
    "    - display.max_columns  \n",
    "\n",
    "      表示列数\n",
    "\n",
    "  - 第二引数\n",
    "\n",
    "    表示する行数・列数\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## 読み込み\n",
    "pandasのメソッドを使用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = Path(PATH_TRAIN)\n",
    "path_test = Path(PATH_TEST)\n",
    "\n",
    "# 訓練データ\n",
    "train_data = pd.read_csv(PATH_TRAIN)\n",
    "\n",
    "# テストデータ\n",
    "test_data = pd.read_csv(PATH_TEST)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## データ内容確認"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### 訓練データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnalysisUtility.display_summary(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### テストデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnalysisUtility.display_summary(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## グループ化\n",
    "- チケットクラス\n",
    "\n",
    "  1 = 1st  \n",
    "  2 = 2nd  \n",
    "  3 = 3rd  \n",
    "\n",
    "- 乗船地\n",
    "\n",
    "  C = Cherbourg  \n",
    "  Q = Queenstown  \n",
    "  S = Southampton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### 訓練データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnalysisUtility.display_categorized_columns(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### テストデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnalysisUtility.display_categorized_columns(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "# データの前処理\n",
    "\n",
    "1. 不要な列の削除\n",
    "2. カテゴリ変数のエンコード\n",
    "3. 欠損値の置換\n",
    "\n",
    "`2と3については順番が不明（個人的には3を先にやった方がよさそう）`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## 特徴量リスト\n",
    "データセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データの列名\n",
    "train_data_columns = [\n",
    "    \"PassengerId\",\n",
    "    \"Survived\",\n",
    "    \"Pclass\",\n",
    "    \"Name\",\n",
    "    \"Sex\",\n",
    "    \"Age\",\n",
    "    \"SibSp\",\n",
    "    \"Parch\",\n",
    "    \"Ticket\",\n",
    "    \"Fare\",\n",
    "    \"Cabin\",\n",
    "    \"Embarked\",\n",
    "]\n",
    "\n",
    "# テストデータの列名\n",
    "test_data_columns = [\n",
    "    \"PassengerId\",\n",
    "    \"Pclass\",\n",
    "    \"Name\",\n",
    "    \"Sex\",\n",
    "    \"Age\",\n",
    "    \"SibSp\",\n",
    "    \"Parch\",\n",
    "    \"Ticket\",\n",
    "    \"Fare\",\n",
    "    \"Cabin\",\n",
    "    \"Embarked\",\n",
    "]\n",
    "\n",
    "# 抽出後の列名（共通）\n",
    "selected_columns = [\n",
    "    \"Pclass\",\n",
    "    \"Sex\",\n",
    "    \"Age\",\n",
    "    \"SibSp\",\n",
    "    \"Parch\",\n",
    "    \"Fare\",\n",
    "    \"Embarked\",\n",
    "]\n",
    "\n",
    "# 共通\n",
    "encode_columns = [\"Pclass\", \"Sex\", \"Embarked\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データ\n",
    "train_data_preprocessed = PreprocessUtility.preprocess_dataset(\n",
    "    train_data,\n",
    "    selected_columns,\n",
    "    encode_columns,\n",
    "    logger,\n",
    ")\n",
    "\n",
    "# データ確認用\n",
    "DisplayUtility.output_divider(\"前処理後の訓練データ\")\n",
    "display(train_data_preprocessed)\n",
    "\n",
    "\n",
    "# テストデータ\n",
    "test_data_preprocessed = PreprocessUtility.preprocess_dataset(\n",
    "    test_data,\n",
    "    selected_columns,\n",
    "    encode_columns,\n",
    "    logger,\n",
    ")\n",
    "\n",
    "# データ確認用\n",
    "DisplayUtility.output_divider(\"前処理後のテストデータ\")\n",
    "display(test_data_preprocessed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## データ用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データ\n",
    "x_train = train_data_preprocessed\n",
    "y_train = train_data[\"Survived\"]\n",
    "\n",
    "# テストデータ\n",
    "x_test = test_data_preprocessed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## 列名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "if x_train.columns.to_numpy().all() and x_test.columns.to_numpy().all():\n",
    "    columns_names = x_train.columns\n",
    "else:\n",
    "    msg = \"NotMatchSizeError: either array has one or more false components.\"\n",
    "    raise FalseComponentError(msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## スケーラー生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正規化\n",
    "scaler = MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "## パラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_logreg = {\"C\": 10.0, \"max_iter\": 1000}\n",
    "\n",
    "params_rf = {\"\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "## モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=1000, random_state=0)\n",
    "rf = RandomForestClassifier(random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "## パイプライン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(scaler, logreg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## ロジスティック回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(pipe, params_logreg_grid, n_jobs=2)\n",
    "\n",
    "search.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_search = search.cv_results_\n",
    "result_search_df = pd.DataFrame(result_search).iloc[:, 4:]\n",
    "result_search_df_rounded = result_search_df.round(3)\n",
    "\n",
    "display(result_search_df_rounded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = search.best_score_\n",
    "print(\"Grid search best score: \", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model: LogisticRegression = search.best_estimator_.named_steps[\"logisticregression\"]\n",
    "\n",
    "y_pred = model.predict(np.array(x_test))\n",
    "\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=[\"Survived\"])\n",
    "y_pred_df_submission = pd.concat([test_data[\"PassengerId\"], y_pred_df], axis=1)\n",
    "\n",
    "display(y_pred_df_submission)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータ\n",
    "params_logreg = {\"C\": 10.0, \"max_iter\": 1000}\n",
    "\n",
    "# モデル生成\n",
    "model_logreg = LogisticRegression(**params_logreg)\n",
    "\n",
    "# 学習\n",
    "model_logreg.fit(x_train, y_train)\n",
    "\n",
    "# 予測\n",
    "y_pred = model_logreg.predict(x_test)\n",
    "y_proba = model_logreg.predict_proba(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df = pd.DataFrame(y_pred, columns=[\"Survived\"])\n",
    "\n",
    "y_pred_df_submission = pd.concat([test_data[\"PassengerId\"], y_pred_df], axis=1)\n",
    "\n",
    "display(y_pred_df_submission)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提出用の形式（PassengerId, Survived）に変更する\n",
    "y_pred_df_with_test_data = PreprocessUtility.insert_dataframe(\n",
    "    df_base=test_data,\n",
    "    df_insert=y_pred_df,\n",
    "    insert_location=1,\n",
    ")\n",
    "\n",
    "display(y_pred_df_with_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各ラベルの予測確率\n",
    "y_proba_df = pd.DataFrame(np.round(y_proba, 3), columns=[\"Survived_0\", \"Survived_1\"])\n",
    "\n",
    "display(y_proba_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 係数\n",
    "model_logreg_coef = pd.DataFrame(\n",
    "    np.round(model_logreg.coef_, 3),\n",
    "    columns=x_train.columns,\n",
    ")\n",
    "\n",
    "display(model_logreg_coef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "CsvUtility.output_csv(y_pred_df_submission, LOGISTIC_REGRESSION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "## ランダムフォレスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータ\n",
    "params_logreg = {}\n",
    "\n",
    "model_rf = RandomForestClassifier()\n",
    "\n",
    "model_rf.fit(x_train, y_train)\n",
    "\n",
    "# 予測\n",
    "y_pred = model_rf.predict(x_test)\n",
    "y_proba = model_rf.predict_proba(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model_rf.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(x_train.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(np.round(model_rf.feature_importances_, 3)).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 係数\n",
    "model_rf_importances = pd.DataFrame(\n",
    "    np.round(model_rf.feature_importances_, 3),\n",
    "    index=x_train.columns,\n",
    ").T\n",
    "\n",
    "display(model_rf_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df = pd.DataFrame(y_pred, columns=[\"Survived\"])\n",
    "\n",
    "y_pred_df_submission = pd.concat([test_data[\"PassengerId\"], y_pred_df], axis=1)\n",
    "\n",
    "display(y_pred_df_submission)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "CsvUtility.output_csv(y_pred_df_submission, RANDOM_FOREST)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-titanic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
